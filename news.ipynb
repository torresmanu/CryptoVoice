{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Descargar stopwords en inglés si no están descargadas\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar el modelo CryptoBERT y el tokenizer correspondiente\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kk08/CryptoBERT\", model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"kk08/CryptoBERT\")\n",
    "\n",
    "# Crear el pipeline de análisis de sentimiento\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Función para limpiar texto (incluye eliminación de stopwords)\n",
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Eliminar íconos y caracteres especiales\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenizar el texto\n",
    "    words = text.lower().split()\n",
    "    # Eliminar stopwords en inglés\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Función para traducir etiquetas de sentimiento\n",
    "def translate_label(label):\n",
    "    label_map = {\n",
    "        'LABEL_0': 'negative',\n",
    "        'LABEL_1': 'positive'\n",
    "    }\n",
    "    return label_map.get(label, 'unknown')\n",
    "\n",
    "# Definir los keywords para cada token, incluyendo los generales bajo 'GENERAL'\n",
    "crypto_keywords = {\n",
    "    'BTC': ['Bitcoin', 'BTC', 'Satoshi', '₿', 'bitcoin', 'btc', 'satoshi'],\n",
    "    'ETH': ['Ethereum', 'ETH', 'Ether', 'Ξ', 'ethereum', 'ether','eth'],\n",
    "    'BNB': ['Binance Coin', 'BNB', 'binance coin', 'bnb'],\n",
    "    'SOL': ['Solana', 'SOL', 'solana', 'sol'],\n",
    "    'LTC': ['Litecoin', 'LTC', 'litecoin', 'ltc'],\n",
    "    'LINK': ['Chainlink', 'LINK', 'chainlink', 'link'],\n",
    "    'MATIC': ['Polygon', 'MATIC', 'polygon', 'matic'],\n",
    "    'ADA': ['Cardano', 'cardano', 'ADA', 'ada'],\n",
    "    'GENERAL': ['crypto', 'cryptocurrency', 'blockchain', 'DeFi', 'NFT', 'altcoin', 'stablecoin',\n",
    "                'hodl', 'fomo', 'fud', 'bullish', 'bearish', 'pump', 'dump', 'moon', 'rekt']\n",
    "}\n",
    "\n",
    "# Función para obtener noticias, analizar el sentimiento y guardar en un archivo CSV\n",
    "def fetch_news_and_analyze_sentiment(start_date, end_date, crypto_keywords):\n",
    "    current_date = start_date\n",
    "\n",
    "    # Crear o abrir el archivo CSV con codificación utf-8\n",
    "    with open('crypto_news_sentiment.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['DATE', 'TOKEN', 'CLEAN TEXT', 'LABEL']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            next_date = current_date + timedelta(days=1)\n",
    "            if next_date > end_date:\n",
    "                next_date = end_date\n",
    "\n",
    "            for token, keywords in crypto_keywords.items():\n",
    "                for query in keywords:\n",
    "                    google_news = GNews(start_date=(current_date.year, current_date.month, current_date.day), \n",
    "                                        end_date=(next_date.year, next_date.month, next_date.day))\n",
    "                    news = google_news.get_news(query)\n",
    "\n",
    "                    if news:\n",
    "                        for article in news:\n",
    "                            title = article.get('title', '')\n",
    "                            description = article.get('description', '')\n",
    "                            combined_text = title + ' ' + description\n",
    "                            \n",
    "                            # Limpiar el texto\n",
    "                            cleaned_text = clean_text(combined_text)\n",
    "                            \n",
    "                            # Analizar el sentimiento\n",
    "                            sentiment_result = classifier(cleaned_text, truncation=True)[0]\n",
    "                            \n",
    "                            # Guardar los datos en el archivo CSV\n",
    "                            writer.writerow({\n",
    "                                'DATE': article.get('published date', ''),\n",
    "                                'TOKEN': token,\n",
    "                                'CLEAN TEXT': cleaned_text,\n",
    "                                'LABEL': translate_label(sentiment_result['label'])\n",
    "                            })\n",
    "                        print(f\"Processed and saved sentiment for {query} (Token: {token}) from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}\")\n",
    "                    else:\n",
    "                        print(f\"No news found for {query} (Token: {token}) from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "            current_date += timedelta(days=2)\n",
    "\n",
    "# Define the date range\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 7, 31)\n",
    "\n",
    "# Fetch news, analyze sentiment, and save results to CSV\n",
    "fetch_news_and_analyze_sentiment(start_date, end_date, crypto_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
